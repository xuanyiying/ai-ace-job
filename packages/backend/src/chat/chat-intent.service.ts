/**
 * Chat Intent Service
 * Handles intent recognition and dispatches to appropriate handlers
 */

import { Injectable, Logger } from '@nestjs/common';
import { PrismaService } from '@/prisma/prisma.service';
import { AIEngineService } from '@/ai-providers/ai-engine.service';
import { ResumeOptimizerService } from '@/resume-optimizer/resume-optimizer.service';
import { AIRequest } from '@/ai-providers/interfaces';
import { PromptScenario } from '@/ai-providers/interfaces/prompt-template.interface';
import { ChatResponse } from './chat.gateway';
import { MessageRole } from '@prisma/client';

export enum ChatIntent {
  OPTIMIZE_RESUME = 'optimize_resume',
  PARSE_RESUME = 'parse_resume',
  MOCK_INTERVIEW = 'mock_interview',
  INTERVIEW_PREDICTION = 'interview_prediction',
  PARSE_JOB_DESCRIPTION = 'parse_job_description',
  GENERAL_CHAT = 'general_chat',
  HELP = 'help',
  UNKNOWN = 'unknown',
}

interface IntentResult {
  intent: ChatIntent;
  confidence: number;
  entities?: Record<string, any>;
}

// In-memory cache for user's resume content (should use Redis in production)
const userResumeCache = new Map<
  string,
  { resumeId: string; content: string; timestamp: number }
>();

@Injectable()
export class ChatIntentService {
  private readonly logger = new Logger(ChatIntentService.name);

  // Intent keywords mapping
  private readonly intentKeywords: Record<ChatIntent, string[]> = {
    [ChatIntent.OPTIMIZE_RESUME]: [
      'ä¼˜åŒ–',
      'æ”¹è¿›',
      'æ¶¦è‰²',
      'æå‡',
      'ä¿®æ”¹',
      'æ”¹å–„',
      'å®Œå–„',
      'optimize',
      'improve',
      'enhance',
      'polish',
      'refine',
      'ä¼˜åŒ–ç®€å†',
      'æ”¹è¿›ç®€å†',
      'æ¶¦è‰²ç®€å†',
      'ç®€å†ä¼˜åŒ–',
    ],
    [ChatIntent.MOCK_INTERVIEW]: [
      'æ¨¡æ‹Ÿé¢è¯•',
      'é¢è¯•',
      'ç»ƒä¹ ',
      'mock',
      'interview',
      'practice',
      'æ¨¡æ‹Ÿ',
      'é¢è¯•è§£å¿§',
    ],
    [ChatIntent.INTERVIEW_PREDICTION]: [
      'é¢è¯•é¢„æµ‹',
      'é¢„æµ‹',
      'é¢˜ç›®',
      'è€ƒé¢˜',
      'prediction',
      'predict',
      'questions',
    ],
    [ChatIntent.PARSE_JOB_DESCRIPTION]: [
      'èŒä½è¾“å…¥',
      'è¾“å…¥èŒä½',
      'è§£æèŒä½',
      'JD',
      'èŒä½',
      'èŒä½æè¿°',
      'job',
      'description',
    ],
    [ChatIntent.PARSE_RESUME]: [
      'è§£æ',
      'åˆ†æ',
      'æŸ¥çœ‹',
      'è¯»å–',
      'parse',
      'analyze',
      'read',
      'extract',
    ],
    [ChatIntent.HELP]: [
      'å¸®åŠ©',
      'æ€ä¹ˆç”¨',
      'å¦‚ä½•',
      'ä½¿ç”¨è¯´æ˜',
      'åŠŸèƒ½',
      'help',
      'how to',
      'guide',
      'tutorial',
    ],
    [ChatIntent.GENERAL_CHAT]: [],
    [ChatIntent.UNKNOWN]: [],
  };

  constructor(
    private readonly prisma: PrismaService,
    private readonly aiEngineService: AIEngineService,
    private readonly resumeOptimizerService: ResumeOptimizerService
  ) {}

  /**
   * Store user's resume content for later use
   */
  async storeUserResumeContent(
    userId: string,
    resumeId: string,
    content: string
  ): Promise<void> {
    userResumeCache.set(userId, {
      resumeId,
      content,
      timestamp: Date.now(),
    });
    this.logger.debug(
      `Stored resume content for user ${userId}, resumeId: ${resumeId}`
    );
  }

  /**
   * Get user's latest resume content
   */
  async getUserResumeContent(
    userId: string
  ): Promise<{ resumeId: string; content: string } | null> {
    // First check cache
    const cached = userResumeCache.get(userId);
    if (cached && Date.now() - cached.timestamp < 3600000) {
      // 1 hour cache
      this.logger.debug(
        `Using cached resume for user ${userId}, content length: ${cached.content.length}`
      );
      return { resumeId: cached.resumeId, content: cached.content };
    }

    // Fallback to database - get user's latest parsed resume
    const resume = await this.prisma.resume.findFirst({
      where: {
        userId,
        parseStatus: 'COMPLETED',
      },
      orderBy: { updatedAt: 'desc' },
    });

    if (resume?.parsedData) {
      const content = this.convertParsedDataToMarkdown(
        resume.parsedData as any
      );
      
      if (!content || content.trim().length === 0) {
        this.logger.warn(
          `Converted resume content is empty for user ${userId}, resumeId: ${resume.id}`
        );
        this.logger.debug(
          `ParsedData structure: ${JSON.stringify(resume.parsedData).substring(0, 200)}`
        );
        return null;
      }

      this.logger.debug(
        `Converted resume to markdown for user ${userId}, content length: ${content.length}`
      );
      
      userResumeCache.set(userId, {
        resumeId: resume.id,
        content,
        timestamp: Date.now(),
      });
      return { resumeId: resume.id, content };
    }

    this.logger.warn(`No parsed resume found for user ${userId}`);
    return null;
  }

  /**
   * Process incoming message and dispatch to appropriate handler
   */
  async processMessage(
    userId: string,
    conversationId: string,
    content: string,
    metadata: Record<string, any> | undefined,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    try {
      // 1. Recognize intent
      const intentResult = this.recognizeIntent(content);
      this.logger.debug(
        `Recognized intent: ${intentResult.intent} (confidence: ${intentResult.confidence})`
      );

      // 2. Dispatch to appropriate handler
      switch (intentResult.intent) {
        case ChatIntent.OPTIMIZE_RESUME:
          await this.handleOptimizeResume(
            userId,
            conversationId,
            content,
            onChunk,
            onComplete
          );
          break;

        case ChatIntent.MOCK_INTERVIEW:
          await this.handleMockInterview(
            userId,
            conversationId,
            content,
            onChunk,
            onComplete
          );
          break;

        case ChatIntent.INTERVIEW_PREDICTION:
          await this.handleInterviewPrediction(
            userId,
            conversationId,
            content,
            onChunk,
            onComplete
          );
          break;

        case ChatIntent.PARSE_JOB_DESCRIPTION:
          await this.handleParseJobDescription(
            userId,
            conversationId,
            content,
            onChunk,
            onComplete
          );
          break;

        case ChatIntent.HELP:
          await this.handleHelp(onChunk, onComplete);
          break;

        case ChatIntent.GENERAL_CHAT:
        default:
          await this.handleGeneralChat(userId, content, onChunk, onComplete);
          break;
      }
    } catch (error) {
      this.logger.error(
        `Intent processing failed: ${error instanceof Error ? error.message : String(error)}`
      );
      const errorMessage =
        error instanceof Error
          ? error.message
          : 'å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚';
      await onComplete(errorMessage, { error: true });
    }
  }

  /**
   * Recognize user intent from message content
   */
  private recognizeIntent(content: string): IntentResult {
    const lowerContent = content.toLowerCase();

    // Check each intent's keywords
    for (const [intent, keywords] of Object.entries(this.intentKeywords)) {
      if (keywords.length === 0) continue;

      for (const keyword of keywords) {
        if (lowerContent.includes(keyword.toLowerCase())) {
          return {
            intent: intent as ChatIntent,
            confidence: 0.9,
          };
        }
      }
    }

    // Default to general chat
    return {
      intent: ChatIntent.GENERAL_CHAT,
      confidence: 0.5,
    };
  }

  /**
   * Handle resume optimization request
   */
  private async handleOptimizeResume(
    userId: string,
    conversationId: string,
    userMessage: string,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    // Get user's resume content
    const resumeData = await this.getUserResumeContent(userId);

    if (!resumeData) {
      await onComplete(
        'è¯·å…ˆä¸Šä¼ æ‚¨çš„ç®€å†æ–‡ä»¶ï¼Œæˆ‘æ‰èƒ½å¸®æ‚¨ä¼˜åŒ–ç®€å†å†…å®¹ã€‚æ‚¨å¯ä»¥ç‚¹å‡»ä¸Šä¼ æŒ‰é’®æˆ–ç›´æ¥æ‹–æ‹½æ–‡ä»¶åˆ°èŠå¤©çª—å£ã€‚',
        { action: 'request_upload' }
      );
      return;
    }

    // Validate resume content
    if (!resumeData.content || resumeData.content.trim().length === 0) {
      this.logger.error(
        `Resume content is empty for user ${userId}, resumeId: ${resumeData.resumeId}`
      );
      await onComplete(
        'ç®€å†å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œä¼˜åŒ–ã€‚è¯·é‡æ–°ä¸Šä¼ ç®€å†æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚',
        { error: true, action: 'content_empty' }
      );
      return;
    }

    this.logger.log(
      `Starting resume optimization for user ${userId}, content length: ${resumeData.content.length}`
    );

    // Send initial message
    onChunk({
      type: 'chunk',
      content: 'æ”¶åˆ°ï¼æ­£åœ¨ä¸ºæ‚¨ä¼˜åŒ–ç®€å†å†…å®¹...\n\n',
      timestamp: Date.now(),
    });

    // Stream optimization
    let fullContent = '';

    try {
      const stream = this.resumeOptimizerService.optimizeResume(
        resumeData.content,
        userId,
        { language: 'zh-CN' }
      );

      for await (const chunk of stream) {
        if (chunk.type === 'chunk' && chunk.content) {
          fullContent += chunk.content;
          onChunk({
            type: 'chunk',
            content: chunk.content,
            timestamp: Date.now(),
          });
        } else if (chunk.type === 'error') {
          throw new Error(chunk.message || 'Optimization failed');
        }
      }

      // Add completion tips
      const tips = this.generateOptimizationTips(fullContent);
      const finalContent = fullContent + tips;

      this.logger.log(
        `Resume optimization completed for user ${userId}, output length: ${fullContent.length}`
      );

      await onComplete(finalContent, {
        type: 'optimization_result',
        resumeId: resumeData.resumeId,
        optimizedContent: fullContent,
      });
    } catch (error) {
      this.logger.error(
        `Resume optimization failed: ${error instanceof Error ? error.message : String(error)}`
      );
      await onComplete(
        `ç®€å†ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}ã€‚è¯·ç¨åé‡è¯•ã€‚`,
        { error: true }
      );
    }
  }

  /**
   * Handle mock interview request
   */
  private async handleMockInterview(
    userId: string,
    conversationId: string,
    userMessage: string,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    const resumeData = await this.getUserResumeContent(userId);

    let systemPrompt =
      'ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„é¢è¯•å®˜ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ç®€å†å†…å®¹ï¼Œä¸ºç”¨æˆ·è¿›è¡Œä¸€æ¬¡æ¨¡æ‹Ÿé¢è¯•ã€‚ä½ å¯ä»¥å…ˆæå‡ºä¸€ä¸ªé¢è¯•é—®é¢˜ï¼Œç„¶åæ ¹æ®ç”¨æˆ·çš„å›ç­”è¿›è¡Œè¿½é—®æˆ–ç‚¹è¯„ã€‚';

    if (resumeData) {
      systemPrompt += `\n\nç”¨æˆ·çš„ç®€å†å†…å®¹å¦‚ä¸‹ï¼š\n${resumeData.content}`;
    } else {
      systemPrompt += '\n\nç”¨æˆ·å°šæœªä¸Šä¼ ç®€å†ã€‚ä½ å¯ä»¥å…ˆå»ºè®®ç”¨æˆ·ä¸Šä¼ ç®€å†ï¼Œæˆ–è€…å…ˆè¿›è¡Œä¸€äº›é€šç”¨çš„é¢è¯•å‡†å¤‡é—®é¢˜ã€‚';
    }

    onChunk({
      type: 'chunk',
      content: 'å‡†å¤‡å¥½äº†ï¼è®©æˆ‘ä»¬å¼€å§‹æ¨¡æ‹Ÿé¢è¯•å§ã€‚\n\n',
      timestamp: Date.now(),
    });

    await this.streamAIResponse(
      userId,
      systemPrompt,
      userMessage,
      onChunk,
      async (finalContent) => {
        await onComplete(finalContent, { type: 'mock_interview' });
      }
    );
  }

  /**
   * Handle interview prediction request
   */
  private async handleInterviewPrediction(
    userId: string,
    conversationId: string,
    userMessage: string,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    const resumeData = await this.getUserResumeContent(userId);

    let systemPrompt =
      'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŒä¸šé¡¾é—®å’Œé¢è¯•ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ç®€å†å†…å®¹ï¼Œé¢„æµ‹é¢è¯•ä¸­å¯èƒ½å‡ºç°çš„é—®é¢˜ï¼Œå¹¶æä¾›ç›¸åº”çš„å›ç­”å»ºè®®å’Œæ ¸å¿ƒè€ƒç‚¹åˆ†æã€‚';

    if (resumeData) {
      systemPrompt += `\n\nç”¨æˆ·çš„ç®€å†å†…å®¹å¦‚ä¸‹ï¼š\n${resumeData.content}`;
    } else {
      systemPrompt += '\n\nç”¨æˆ·å°šæœªä¸Šä¼ ç®€å†ã€‚ä½ å¯ä»¥å…ˆæä¾›ä¸€äº›å¸¸è§çš„é€šç”¨é¢è¯•é—®é¢˜é¢„æµ‹ï¼Œå¹¶å»ºè®®ç”¨æˆ·ä¸Šä¼ ç®€å†ä»¥è·å–æ›´ç²¾å‡†çš„é¢„æµ‹ã€‚';
    }

    onChunk({
      type: 'chunk',
      content: 'æ­£åœ¨ä¸ºæ‚¨åˆ†æç®€å†å¹¶é¢„æµ‹é¢è¯•é—®é¢˜...\n\n',
      timestamp: Date.now(),
    });

    await this.streamAIResponse(
      userId,
      systemPrompt,
      userMessage,
      onChunk,
      async (finalContent) => {
        await onComplete(finalContent, { type: 'interview_prediction' });
      }
    );
  }

  /**
   * Handle job description parsing request
   */
  private async handleParseJobDescription(
    userId: string,
    conversationId: string,
    userMessage: string,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    const systemPrompt =
      'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŒä½åˆ†æä¸“å®¶ã€‚è¯·å¸®ç”¨æˆ·è§£æèŒä½æè¿°ï¼ˆJDï¼‰ï¼Œæå–å‡ºæ ¸å¿ƒèŒè´£ã€æŠ€èƒ½è¦æ±‚ã€ä»»èŒèµ„æ ¼ç­‰å…³é”®ä¿¡æ¯ï¼Œå¹¶ç»™å‡ºç®€å†æŠ•é€’çš„å»ºè®®ã€‚';

    onChunk({
      type: 'chunk',
      content: 'å¥½çš„ï¼Œè¯·æä¾›èŒä½æè¿°ä¿¡æ¯ï¼Œæˆ‘å°†ä¸ºæ‚¨è¿›è¡Œæ·±åº¦è§£æã€‚\n\n',
      timestamp: Date.now(),
    });

    await this.streamAIResponse(
      userId,
      systemPrompt,
      userMessage,
      onChunk,
      async (finalContent) => {
        await onComplete(finalContent, { type: 'parse_job_description' });
      }
    );
  }

  /**
   * Helper to stream AI response
   */
  private async streamAIResponse(
    userId: string,
    systemPrompt: string,
    userMessage: string,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (finalContent: string) => Promise<void>
  ): Promise<void> {
    const prompt = `${systemPrompt}\n\nç”¨æˆ·æ¶ˆæ¯ï¼š${userMessage}\n\nè¯·ç”¨ä¸­æ–‡å›å¤ï¼š`;

    try {
      const aiRequest: AIRequest = {
        model: '',
        prompt,
        temperature: 0.7,
        maxTokens: 1500,
      };

      let fullContent = '';
      const stream = this.aiEngineService.stream(
        aiRequest,
        userId,
        PromptScenario.RESUME_OPTIMIZATION,
        'zh-CN'
      );

      for await (const chunk of stream) {
        if (chunk.content) {
          fullContent += chunk.content;
          onChunk({
            type: 'chunk',
            content: chunk.content,
            timestamp: Date.now(),
          });
        }
      }

      await onComplete(fullContent);
    } catch (error) {
      this.logger.error(
        `AI streaming failed: ${error instanceof Error ? error.message : String(error)}`
      );
      await onComplete('æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚');
    }
  }

  /**
   * Handle help request
   */
  private async handleHelp(
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    const helpContent = `
## ğŸ¯ æˆ‘å¯ä»¥å¸®æ‚¨åšä»€ä¹ˆï¼Ÿ

### ğŸ“„ ç®€å†ä¼˜åŒ–
- ä¸Šä¼ æ‚¨çš„ç®€å†ï¼Œè¯´"ä¼˜åŒ–ç®€å†"æˆ–"ç®€å†ä¼˜åŒ–"ï¼Œæˆ‘ä¼šå¸®æ‚¨æ¶¦è‰²å†…å®¹ï¼Œæå‡ç«äº‰åŠ›ã€‚

### ğŸ“ˆ é¢è¯•é¢„æµ‹
- åŸºäºæ‚¨çš„èƒŒæ™¯å’Œç›®æ ‡èŒä½ï¼Œé¢„æµ‹é¢è¯•ä¸­å¯èƒ½å‡ºç°çš„é—®é¢˜ï¼Œå¹¶æä¾›æ ¸å¿ƒè€ƒç‚¹åˆ†æã€‚

### ğŸ­ æ¨¡æ‹Ÿé¢è¯•
- è¿›å…¥å®æˆ˜æ¼”ç»ƒï¼Œæˆ‘å°†ä½œä¸ºé¢è¯•å®˜ä¸æ‚¨å¯¹è¯ï¼Œæä¾›å³æ—¶åé¦ˆå’Œæ”¹è¿›å»ºè®®ã€‚

### ğŸ’¼ èŒä½è¾“å…¥
- ç²˜è´´èŒä½æè¿°ï¼ˆJDï¼‰ï¼Œæˆ‘å°†ä¸ºæ‚¨æ·±åº¦è§£ææ ¸å¿ƒéœ€æ±‚ï¼Œå¹¶ç»™å‡ºé’ˆå¯¹æ€§çš„æŠ•é€’å»ºè®®ã€‚

### ğŸ’¡ ä½¿ç”¨æŠ€å·§
1. **ä¸Šä¼ ç®€å†**ï¼šç‚¹å‡»ä¸Šä¼ æŒ‰é’®æˆ–æ‹–æ‹½æ–‡ä»¶å¼€å§‹ã€‚
2. **é€‰æ‹©åŠŸèƒ½**ï¼šç‚¹å‡»æ¬¢è¿å¡ç‰‡ä¸Šçš„åŠŸèƒ½å›¾æ ‡ï¼Œæˆ–ç›´æ¥åœ¨è¾“å…¥æ¡†ä¸­è¯´æ˜æ‚¨çš„éœ€æ±‚ã€‚
3. **æ·±åº¦å¯¹è¯**ï¼šæ‚¨å¯ä»¥å¯¹æˆ‘çš„å›ç­”è¿›è¡Œè¿½é—®ï¼Œè·å–æ›´è¯¦ç»†çš„å»ºè®®ã€‚

æœ‰ä»»ä½•é—®é¢˜ï¼Œéšæ—¶é—®æˆ‘ï¼
`;

    await onComplete(helpContent, { type: 'help' });
  }

  /**
   * Handle general chat with AI
   */
  private async handleGeneralChat(
    userId: string,
    content: string,
    onChunk: (chunk: ChatResponse) => void,
    onComplete: (
      finalContent: string,
      metadata?: Record<string, any>
    ) => Promise<void>
  ): Promise<void> {
    // Check if user has resume context
    const resumeData = await this.getUserResumeContent(userId);

    let systemPrompt =
      'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†ä¼˜åŒ–åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·æ”¹è¿›ç®€å†å†…å®¹ã€æä¾›æ±‚èŒå»ºè®®ã€‚';

    if (resumeData) {
      systemPrompt += `\n\nç”¨æˆ·å·²ä¸Šä¼ ç®€å†ï¼Œä»¥ä¸‹æ˜¯ç®€å†å†…å®¹æ‘˜è¦ï¼š\n${resumeData.content.substring(0, 500)}...`;
    } else {
      systemPrompt +=
        '\n\nç”¨æˆ·å°šæœªä¸Šä¼ ç®€å†ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ç®€å†ç›¸å…³é—®é¢˜ï¼Œå»ºè®®ä»–ä»¬å…ˆä¸Šä¼ ç®€å†ã€‚';
    }

    const prompt = `${systemPrompt}\n\nç”¨æˆ·æ¶ˆæ¯ï¼š${content}\n\nè¯·ç”¨ä¸­æ–‡å›å¤ï¼š`;

    try {
      const aiRequest: AIRequest = {
        model: '',
        prompt,
        temperature: 0.7,
        maxTokens: 1000,
      };

      let fullContent = '';
      const stream = this.aiEngineService.stream(
        aiRequest,
        userId,
        PromptScenario.RESUME_OPTIMIZATION,
        'zh-CN'
      );

      for await (const chunk of stream) {
        if (chunk.content) {
          fullContent += chunk.content;
          onChunk({
            type: 'chunk',
            content: chunk.content,
            timestamp: Date.now(),
          });
        }
      }

      await onComplete(fullContent, { type: 'general_chat' });
    } catch (error) {
      this.logger.error(
        `General chat failed: ${error instanceof Error ? error.message : String(error)}`
      );

      // Fallback response
      let fallbackResponse = 'æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚';

      if (!resumeData) {
        fallbackResponse =
          'æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ç®€å†ä¼˜åŒ–åŠ©æ‰‹ã€‚è¯·å…ˆä¸Šä¼ æ‚¨çš„ç®€å†ï¼Œç„¶åæˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n\n1. ğŸ“ ä¼˜åŒ–ç®€å†å†…å®¹\n2. ğŸ’¡ æä¾›æ±‚èŒå»ºè®®\n3. ğŸ¯ åˆ†æç®€å†äº®ç‚¹\n\nè¯·ç‚¹å‡»ä¸Šä¼ æŒ‰é’®å¼€å§‹å§ï¼';
      }

      await onComplete(fallbackResponse, { type: 'fallback' });
    }
  }

  /**
   * Generate optimization tips based on content
   */
  private generateOptimizationTips(content: string): string {
    const sections = [
      { key: 'åŸºæœ¬ä¿¡æ¯', tip: 'âœ… å·²ä¼˜åŒ–åŸºæœ¬ä¿¡æ¯ï¼Œå¢å¼ºäº†ä¸ªäººè”ç³»æ–¹å¼çš„æ’ç‰ˆ' },
      { key: 'ä¸“ä¸šæ€»ç»“', tip: 'âœ… å·²ä¼˜åŒ–ä¸“ä¸šæ€»ç»“ï¼Œæå‡äº†æ ¸å¿ƒç«äº‰åŠ›çš„è¡¨è¾¾' },
      { key: 'å·¥ä½œç»å†', tip: 'âœ… å·²ä¼˜åŒ–å·¥ä½œç»å†ï¼Œå¼ºåŒ–äº†é‡åŒ–æˆæœå’ŒæŠ€æœ¯å…³é”®è¯' },
      { key: 'æ•™è‚²èƒŒæ™¯', tip: 'âœ… å·²ä¼˜åŒ–æ•™è‚²èƒŒæ™¯ï¼Œæ•´ç†äº†å­¦æœ¯æˆå°±å’Œè£èª‰' },
      {
        key: 'é¡¹ç›®ç»éªŒ',
        tip: 'âœ… å·²ä¼˜åŒ–é¡¹ç›®ç»éªŒï¼Œçªå‡ºäº†ä¸ªäººåœ¨é¡¹ç›®ä¸­çš„æ ¸å¿ƒè´¡çŒ®',
      },
      { key: 'æŠ€èƒ½åˆ—è¡¨', tip: 'âœ… å·²ä¼˜åŒ–æŠ€èƒ½åˆ—è¡¨ï¼ŒæŒ‰ä¸“ä¸šç±»åˆ«è¿›è¡Œäº†ç»“æ„åŒ–åˆ†ç±»' },
    ];

    const activeTips: string[] = [];
    sections.forEach((section) => {
      if (content.includes(section.key)) {
        activeTips.push(section.tip);
      }
    });

    if (activeTips.length === 0) return '';

    return (
      '\n\n---\n' +
      activeTips.join('\n') +
      '\n\n**âœ¨ ç®€å†ä¼˜åŒ–å®Œæˆï¼æ‚¨å¯ä»¥ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æŸ¥çœ‹å¯¹æ¯”æˆ–ä¸‹è½½ç»“æœã€‚**'
    );
  }

  /**
   * Convert parsed resume data to markdown
   */
  private convertParsedDataToMarkdown(parsedData: any): string {
    if (!parsedData) return '';

    const parts: string[] = [];

    // Handle different parsedData formats
    // Some resumes might have markdown field directly
    if (typeof parsedData === 'string') {
      return parsedData;
    }

    if (parsedData.markdown && typeof parsedData.markdown === 'string') {
      return parsedData.markdown;
    }

    if (parsedData.extractedText && typeof parsedData.extractedText === 'string') {
      return parsedData.extractedText;
    }

    // Personal Info
    if (parsedData.personalInfo) {
      const info = parsedData.personalInfo;
      parts.push(`# ${info.name || 'ç®€å†'}\n`);
      if (info.email) parts.push(`ğŸ“§ ${info.email}`);
      if (info.phone) parts.push(`ğŸ“± ${info.phone}`);
      if (info.location) parts.push(`ğŸ“ ${info.location}`);
      parts.push('');
    }

    // Summary
    if (parsedData.summary) {
      parts.push('## ä¸“ä¸šæ€»ç»“\n');
      parts.push(parsedData.summary);
      parts.push('');
    }

    // Experience
    if (parsedData.experience?.length > 0) {
      parts.push('## å·¥ä½œç»å†\n');
      parsedData.experience.forEach((exp: any) => {
        parts.push(`### ${exp.position} @ ${exp.company}`);
        parts.push(`*${exp.startDate} - ${exp.endDate || 'è‡³ä»Š'}*\n`);
        if (exp.description?.length > 0) {
          exp.description.forEach((desc: string) => parts.push(`- ${desc}`));
        }
        parts.push('');
      });
    }

    // Education
    if (parsedData.education?.length > 0) {
      parts.push('## æ•™è‚²èƒŒæ™¯\n');
      parsedData.education.forEach((edu: any) => {
        parts.push(`### ${edu.degree} - ${edu.field}`);
        parts.push(`${edu.institution} | ${edu.startDate} - ${edu.endDate}`);
        parts.push('');
      });
    }

    // Skills
    if (parsedData.skills?.length > 0) {
      parts.push('## æŠ€èƒ½åˆ—è¡¨\n');
      parts.push(parsedData.skills.join('ã€'));
      parts.push('');
    }

    // Projects
    if (parsedData.projects?.length > 0) {
      parts.push('## é¡¹ç›®ç»éªŒ\n');
      parsedData.projects.forEach((proj: any) => {
        parts.push(`### ${proj.name}`);
        parts.push(proj.description);
        if (proj.technologies?.length > 0) {
          parts.push(`**æŠ€æœ¯æ ˆ**: ${proj.technologies.join(', ')}`);
        }
        parts.push('');
      });
    }

    const result = parts.join('\n');
    
    // If we couldn't extract any content, try to stringify the data
    if (!result || result.trim().length === 0) {
      this.logger.warn('Could not convert parsedData to markdown, using JSON fallback');
      return JSON.stringify(parsedData, null, 2);
    }

    return result;
  }
}
